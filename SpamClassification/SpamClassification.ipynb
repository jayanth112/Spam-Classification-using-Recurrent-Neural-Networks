{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\S.Gyanjoth Singh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\S.Gyanjoth Singh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\S.Gyanjoth Singh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\S.Gyanjoth Singh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\S.Gyanjoth Singh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\S.Gyanjoth Singh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\S.Gyanjoth Singh\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\S.Gyanjoth Singh\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\S.Gyanjoth Singh\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\S.Gyanjoth Singh\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\S.Gyanjoth Singh\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\S.Gyanjoth Singh\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#load text processing NLP classes and packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #loading tfidf vector\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, Bidirectional, GRU\n",
    "from keras.models import Sequential, load_model, Model\n",
    "import pickle\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define global variables for text processing such as lemmatization and stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to clean text by removing stop words and other special symbols\n",
    "def cleanText(doc):\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...       ...                                                ...\n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568      ham               Will ü b going to esplanade fr home?\n",
       "5569      ham  Pity, * was in mood for that. So...any other s...\n",
       "5570      ham  The guy did some bitching but I acted like i'd...\n",
       "5571      ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load spam dataset\n",
    "dataset = pd.read_csv(\"Dataset/SPAM.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Cleaning Completed\n"
     ]
    }
   ],
   "source": [
    "#read all messages and clean them\n",
    "data = dataset.values\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(len(data)):#loop all messages\n",
    "    msg = data[i,1]\n",
    "    msg = msg.strip(\"\\n\").strip().lower()#read msg\n",
    "    label = data[i,0]#read labels\n",
    "    if label == \"ham\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    msg = cleanText(msg)#clean message\n",
    "    X.append(msg)\n",
    "    Y.append(label)\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "print(\"Dataset Cleaning Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account</th>\n",
       "      <th>actual</th>\n",
       "      <th>aight</th>\n",
       "      <th>alreadi</th>\n",
       "      <th>also</th>\n",
       "      <th>alway</th>\n",
       "      <th>amp</th>\n",
       "      <th>anoth</th>\n",
       "      <th>answer</th>\n",
       "      <th>anyth</th>\n",
       "      <th>...</th>\n",
       "      <th>worri</th>\n",
       "      <th>would</th>\n",
       "      <th>xxx</th>\n",
       "      <th>ya</th>\n",
       "      <th>ye</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>yo</th>\n",
       "      <th>yup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.376148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      account  actual  aight   alreadi  also  alway  amp  anoth  answer  \\\n",
       "0         0.0     0.0    0.0  0.000000   0.0    0.0  0.0    0.0     0.0   \n",
       "1         0.0     0.0    0.0  0.000000   0.0    0.0  0.0    0.0     0.0   \n",
       "2         0.0     0.0    0.0  0.000000   0.0    0.0  0.0    0.0     0.0   \n",
       "3         0.0     0.0    0.0  0.376148   0.0    0.0  0.0    0.0     0.0   \n",
       "4         0.0     0.0    0.0  0.000000   0.0    0.0  0.0    0.0     0.0   \n",
       "...       ...     ...    ...       ...   ...    ...  ...    ...     ...   \n",
       "5567      0.0     0.0    0.0  0.000000   0.0    0.0  0.0    0.0     0.0   \n",
       "5568      0.0     0.0    0.0  0.000000   0.0    0.0  0.0    0.0     0.0   \n",
       "5569      0.0     0.0    0.0  0.000000   0.0    0.0  0.0    0.0     0.0   \n",
       "5570      0.0     0.0    0.0  0.000000   0.0    0.0  0.0    0.0     0.0   \n",
       "5571      0.0     0.0    0.0  0.000000   0.0    0.0  0.0    0.0     0.0   \n",
       "\n",
       "      anyth  ...  worri  would  xxx   ya   ye  yeah  year  yet   yo  yup  \n",
       "0       0.0  ...    0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "1       0.0  ...    0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "2       0.0  ...    0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "3       0.0  ...    0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "4       0.0  ...    0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "...     ...  ...    ...    ...  ...  ...  ...   ...   ...  ...  ...  ...  \n",
       "5567    0.0  ...    0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5568    0.0  ...    0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5569    0.0  ...    0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5570    0.0  ...    0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5571    0.0  ...    0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5572 rows x 300 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating fixed size vector model from messages\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=300)\n",
    "X = tfidf_vectorizer.fit_transform(X).toarray()\n",
    "data = pd.DataFrame(X, columns=tfidf_vectorizer.get_feature_names())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Features = [[-0.08020853 -0.07303585 -0.07580714 ... -0.08855605 -0.07462695\n",
      "  -0.08389389]\n",
      " [-0.08020853 -0.07303585 -0.07580714 ... -0.08855605 -0.07462695\n",
      "  -0.08389389]\n",
      " [-0.08020853 -0.07303585 -0.07580714 ... -0.08855605 -0.07462695\n",
      "  -0.08389389]\n",
      " ...\n",
      " [-0.08020853 -0.07303585 -0.07580714 ... -0.08855605 -0.07462695\n",
      "  -0.08389389]\n",
      " [-0.08020853 -0.07303585 -0.07580714 ... -0.08855605 -0.07462695\n",
      "  -0.08389389]\n",
      " [-0.08020853 -0.07303585 -0.07580714 ... -0.08855605 -0.07462695\n",
      "  -0.08389389]]\n"
     ]
    }
   ],
   "source": [
    "#normalizing features\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "print(\"Normalized Features = \"+str(X))\n",
    "Y = to_categorical(Y)\n",
    "X = np.reshape(X, (X.shape[0], 30, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 80% Records = 4457\n",
      "Testing 20% Records = 1115\n"
     ]
    }
   ],
   "source": [
    "#split dataset to train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(\"Training 80% Records = \"+str(X_train.shape[0]))\n",
    "print(\"Testing 20% Records = \"+str(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 30, 64)            19200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 30, 64)            18624     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 64)                18624     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 63,150\n",
      "Trainable params: 63,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define LSTM algorithm\n",
    "lstm_model = Sequential()#defining deep learning sequential object\n",
    "#adding LSTM layer with 100 filters to filter given input X train data to select relevant features\n",
    "lstm_model.add(LSTM(64,input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "#adding dropout layer to remove irrelevant features\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Bidirectional(GRU(32)))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "#adding another layer\n",
    "lstm_model.add(Dense(100, activation='relu'))\n",
    "#defining output layer for prediction\n",
    "lstm_model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "#compile the model\n",
    "lstm_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4457/4457 [==============================] - 56s 13ms/step - loss: 0.2846 - accuracy: 0.8932 - val_loss: 0.2313 - val_accuracy: 0.9184\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.23133, saving model to model/lstm_weights.hdf5\n",
      "Epoch 2/3\n",
      "4457/4457 [==============================] - 58s 13ms/step - loss: 0.2258 - accuracy: 0.9172 - val_loss: 0.2051 - val_accuracy: 0.9354\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.23133 to 0.20510, saving model to model/lstm_weights.hdf5\n",
      "Epoch 3/3\n",
      "4457/4457 [==============================] - 59s 13ms/step - loss: 0.1960 - accuracy: 0.9309 - val_loss: 0.1790 - val_accuracy: 0.9471\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.20510 to 0.17903, saving model to model/lstm_weights.hdf5\n",
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/5\n",
      "4457/4457 [==============================] - 37s 8ms/step - loss: 0.1696 - accuracy: 0.9394 - val_loss: 0.1658 - val_accuracy: 0.9444\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16578, saving model to model/lstm_weights.hdf5\n",
      "Epoch 2/5\n",
      "4457/4457 [==============================] - 33s 8ms/step - loss: 0.1614 - accuracy: 0.9468 - val_loss: 0.1712 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.16578\n",
      "Epoch 3/5\n",
      "4457/4457 [==============================] - 34s 8ms/step - loss: 0.1505 - accuracy: 0.9470 - val_loss: 0.1715 - val_accuracy: 0.9453\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.16578\n",
      "Epoch 4/5\n",
      "4457/4457 [==============================] - 34s 8ms/step - loss: 0.1399 - accuracy: 0.9560 - val_loss: 0.1715 - val_accuracy: 0.9471\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.16578\n",
      "Epoch 5/5\n",
      "4457/4457 [==============================] - 34s 8ms/step - loss: 0.1401 - accuracy: 0.9529 - val_loss: 0.1700 - val_accuracy: 0.9471\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.16578\n",
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/10\n",
      "4457/4457 [==============================] - 21s 5ms/step - loss: 0.1268 - accuracy: 0.9610 - val_loss: 0.1514 - val_accuracy: 0.9525\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15144, saving model to model/lstm_weights.hdf5\n",
      "Epoch 2/10\n",
      "4457/4457 [==============================] - 21s 5ms/step - loss: 0.1208 - accuracy: 0.9583 - val_loss: 0.1661 - val_accuracy: 0.9507\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.15144\n",
      "Epoch 3/10\n",
      "4457/4457 [==============================] - 22s 5ms/step - loss: 0.1138 - accuracy: 0.9621 - val_loss: 0.1597 - val_accuracy: 0.9516\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.15144\n",
      "Epoch 4/10\n",
      "4457/4457 [==============================] - 22s 5ms/step - loss: 0.1098 - accuracy: 0.9659 - val_loss: 0.1649 - val_accuracy: 0.9480\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.15144\n",
      "Epoch 5/10\n",
      "4457/4457 [==============================] - 23s 5ms/step - loss: 0.1104 - accuracy: 0.9634 - val_loss: 0.1692 - val_accuracy: 0.9435\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.15144\n",
      "Epoch 6/10\n",
      "4457/4457 [==============================] - 21s 5ms/step - loss: 0.0996 - accuracy: 0.9688 - val_loss: 0.1823 - val_accuracy: 0.9498\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.15144\n",
      "Epoch 7/10\n",
      "4457/4457 [==============================] - 21s 5ms/step - loss: 0.0993 - accuracy: 0.9695 - val_loss: 0.1672 - val_accuracy: 0.9516\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.15144\n",
      "Epoch 8/10\n",
      "4457/4457 [==============================] - 21s 5ms/step - loss: 0.0959 - accuracy: 0.9690 - val_loss: 0.1888 - val_accuracy: 0.9444\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.15144\n",
      "Epoch 9/10\n",
      "4457/4457 [==============================] - 21s 5ms/step - loss: 0.1007 - accuracy: 0.9684 - val_loss: 0.1720 - val_accuracy: 0.9543\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.15144\n",
      "Epoch 10/10\n",
      "4457/4457 [==============================] - 21s 5ms/step - loss: 0.0949 - accuracy: 0.9695 - val_loss: 0.1725 - val_accuracy: 0.9525\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.15144\n"
     ]
    }
   ],
   "source": [
    "#tuning LSTM using hyper parameters\n",
    "param_grid = {\n",
    "    'epochs': [3, 5, 10],\n",
    "    'batch_size': [16, 32, 64],    \n",
    "}\n",
    "best_model = None\n",
    "best_score = 0\n",
    "best_param = None\n",
    "#training LSTM using tuning parameters\n",
    "result = [dict(zip(param_grid.keys(), v)) for v in zip(*param_grid.values())]\n",
    "for i in range(len(result)):\n",
    "    epoch = result[i]['epochs']\n",
    "    batch_size  = result[i]['batch_size']\n",
    "    model_check_point = ModelCheckpoint(filepath='model/lstm_weights.hdf5', verbose = 1, save_best_only = True)\n",
    "    lstm_model.fit(X_train, y_train, batch_size = batch_size, epochs = epoch, validation_data=(X_test, y_test), callbacks=[model_check_point], verbose=1)\n",
    "    loss, accuracy = lstm_model.evaluate(X_test, y_test, verbose = 0)\n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_model = lstm_model\n",
    "        best_param = \"Epochs: \"+str(epoch)+\" Batch Size: \"+str(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9524663686752319\n",
      "Best Tuning Params: Epochs: 10 Batch Size: 64\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: \"+str(best_score))\n",
    "print(\"Best Tuning Params: \"+str(best_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Performance Measure\n",
      "Accuracy: 95.24663677130046\n",
      "Precision: 91.82831776594726\n",
      "Recall: 86.67269906928645\n",
      "FSCORE: 89.00813764240874\n"
     ]
    }
   ],
   "source": [
    "#evaluating best model performance\n",
    "predict = best_model.predict(X_test)#performing prediction on test data using best model\n",
    "predict = np.argmax(predict, axis=1)\n",
    "y_test1 = np.argmax(y_test, axis=1)\n",
    "a = accuracy_score(y_test1,predict)*100 #calculate accuracy and other metrics using original labels and predicted labels\n",
    "p = precision_score(y_test1, predict,average='macro') * 100\n",
    "r = recall_score(y_test1, predict,average='macro') * 100\n",
    "f = f1_score(y_test1, predict,average='macro') * 100\n",
    "print(\"Best Model Performance Measure\")\n",
    "print(\"Accuracy: \"+str(a))\n",
    "print(\"Precision: \"+str(p))\n",
    "print(\"Recall: \"+str(r))\n",
    "print(\"FSCORE: \"+str(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
